---
title: "Hands On ‚Äì Tidy & Transform / Analyze (Einheiten 9 und 10)"
editor: visual
bibliography: references.bib
---

```{r, echo = TRUE, results = 'hide', message = FALSE, warning = FALSE}
library(tidyverse)
library(here)

dat_full <- read_csv(here("data/raw/dat_full.csv"))
```

Bei Bedarf finden sich hier nochmal die Slides zur EH9:

::: {=html}
<iframe src="../01_slides/EH_9.html"
        width="100%"
        height="500"
        style="border:0; display:block; margin: 0 0 2rem 0;"></iframe>
:::

Und hier die Slides zur EH10:

::: {=html}
<iframe src="../01_slides/EH_10.html"
        width="100%"
        height="500"
        style="border:0; display:block; margin: 0 0 2rem 0;"></iframe>
:::

# Lernziele

‚úÖ Dplyr: Repetition

‚úÖ Duplizierte Werte Entfernen

‚úÖ Long & Wide Transformationen

‚úÖ Datenqualit√§tsindkatoren berechnen

‚úÖ Ausreisseranalysen

‚úÖ Tabellen

::: callout-important
F√ºr die heutigen √úbungen ben√∂tigen wir `dat_full`!
:::

# **Duplizierte Werte entfernen**

In vielen Datens√§tzen kann es vorkommen, dass Werte mehrfach auftreten (z. B. wenn eine ID mehrmals vorkommt). Um dies zu bereinigen, m√ºssen wir zun√§chst die Duplikate identifizieren und sie anschliessend bearbeiten oder entfernen.

Im n√§chsten Schritt duplizieren wir exemplarisch drei zuf√§llige Zeilen aus unserem Datensatz **dat_full**. Diese k√ºnstlich erzeugten Duplikate werden wir danach wieder bereinigen.

F√ºhre diesen Code-Chunk aus: Danach sollte dein **dat_full** 162 Beobachtungen enthalten.

```{r}
set.seed(123)   

dup_rows <- dat_full |> 
  slice_sample(n = 3)

dat_full <- bind_rows(dat_full, dup_rows)

```

Es gibt mehrere Arten und Weisen, Duplikate zu identifizieren. Verwende die `dplyr`-Funktion `count()`, um die einzelnen IDs zu z√§hlen. Gib dir anschliessend mit `filter()` diejenigen aus, die mehr als einmal vorkommen.

```{r echo = TRUE, eval = FALSE}
dat_full |> 
  count(XXXX) |> 
  filter(XXX)


```

```{r}
dat_full |>
  count(code) |> 
  filter(n > 1)
```

Entferne die Duplikate wieder, indem du z. B. `unique()` auf den Datensatz anwendest. `unique()` entfernt vollst√§ndig identische Zeilen und beh√§lt nur eine davon.

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE}
dat_full <- unique(dat_full)
```
:::

::: {.callout-note collapse="true"}
Alternative: Falls du Duplikate nur auf Basis einer bestimmten Variable (z. B. id) entfernen m√∂chtest, kannst du duplicated() verwenden. Dadurch bleibt pro ID nur die erste Zeile erhalten.

```{r echo = TRUE, eval = TRUE}
dat_full <- dat_full[!duplicated(dat_full$code), ]

```
:::

# Dplyr: Code korrigieren und verbessern

**Dieser Code funktioniert nicht. Korrigiere ihn. Achte dabei auf Syntaxfehler.**

```{r, echo = TRUE, eval = FALSE}
dat_full |>
  select(code, post, strategies)
  filter(strtegies = "1")



```

::: {.callout-note collapse="true" title="L√∂sung"}
```{r echo = TRUE, eval = TRUE}
dat_full |>
  select(code, post, strategies) |>
  filter(strategies == 1)
```
:::

**Dieser Code funktioniert, ist jedoch sehr umst√§ndlich und ineffizient. Verwende `dplyr`-Funktionen, um ihn k√ºrzer und √ºbersichtlicher zu gestalten.**

```{r echo = TRUE, eval = FALSE}
dat_full |>
  summarise(
    Gesamtmittelwert = mean(
      c(
        question1, question2, question3, question4, question5,
        question6, question7, question8, question9, question10,
        question11, question12, question13, question14, question15,
        question16, question17, question18
      )
    )
  )

```

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

dat_full |>
  summarise(
    Gesamtmean = mean(across(starts_with("question")), na.rm = TRUE)
  )

```
:::

# Wide to Long:

## √úbungen zur Datenkonversion

üìñ[R4DS - Kapitel 5.3](https://r4ds.hadley.nz/data-tidy.html#sec-pivoting)

üìñ[Einf√ºhrung in R - Kapitel 3.1.3](https://methodenlehre.github.io/einfuehrung-in-R/chapters/03-data_frames.html#manuelle-konversion-von-wide-zu-long)

Die folgenden Schritte dienen der Konversion von Daten zwischen dem Wide- und Long-Format unter Verwendung des `Tidyverse`-Pakets in R.

-   Mit der Funktion **`pivot_longer()`** Spalten der Leistungseinsch√§tzung in Long-Format umwandeln und in neuen Datensatz speichern, **nur f√ºr √úbungszwecke**.

::: {.callout-note collapse="true" title="L√∂sung"}
```{r echo = TRUE, eval = TRUE}
df_long <- dat_full |>
  pivot_longer(
    cols = c(pre1, pre2, pre3, pre4, post),
    names_to = "time_rating",
    values_to = "rating"
  )
```
:::

-   Wandle den Datensatz noch mal in das Long Format um, aber nur basierend auf den Spalten **`pre1`** und **`pre4`** **Long-Format wird f√ºr 2x3 ANOVA aus Grinschgl et al. (2020) ben√∂tigt; f√ºr Vergleich der 2 Messzeitpunkte in dieser Analysen, ben√∂tigen wir diese im Long-Format.**

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}
df_long <- dat_full |>
  pivot_longer(
    cols = c(pre1, pre4),
    names_to = "time_rating",
    values_to = "rating"
  )
```
:::

-   Wandle die Spalte **`time_rating`** in einen Factor um ‚Äì\> **auch notwendig f√ºr Analysen**.

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

df_long$time_rating <- factor(df_long$time_rating)
```
:::

-   Speichere den Long Datensatz als **`csv.`** Datei ab. Achtung: achte auf das row.names() argument, um keine unn√∂tige Spalte zu generieren.

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = FALSE}

write.csv(df_long, "processed/data_long.csv", row.names = FALSE)

```
:::

-   Verwende **`pivot_wider`** unter Angabe von **`names_from`** und **`values_from`** um den Datensatz wieder in das Wide-Format zu bringen. ‚Äì\> **nur f√ºr √úbungszwecke.**

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

df_wide <- df_long |> 
  pivot_wider(names_from = time_rating,
              values_from = rating)
```
:::

# Datenqualit√§t

## Skewness und Kurtosis

Die folgenden Schritte dienen der Vorbereitung und deskriptiven Analyse von Daten in R.

-   **Installieren und laden des Pakets** {psych}: Stelle sicher, dass das Statis Paket `psych`f√ºr die weiteren Analysen geladen ist.

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

# install.packages("psych")
library(psych)
```
:::

-   Berechne die Schiefe und die Kurtosis der Variablen "√ñffnungen des Modellfensters" (`mean_rl_all`) f√ºr die drei Gruppen. Verwende hierf√ºr die `dplyr`-Funktionen `group_by()` und `summarize()`.

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

summary_table <- dat_full |>
  group_by(group_all) |>
  summarize(schiefe = skew(mean_rl_all),
            w√∂lbung = kurtosi(mean_rl_all))

summary_table


```
:::

## Residuen

üìñSiehe auch: [Normalverteilung der Residuen bei der Regression in R testen](https://bjoernwalther.com/normalverteilung-der-residuen-bei-der-regression-in-r-testen/)

F√ºr die folgenden Schritte berechnen wir zuerst ein Regressionsmodell:

-   **Spezifikation des Regressionsmodells:** Verwende die Funktion `lm()` um ein Regressionsmodell zwischen Cognitive Offloading (`mean_rl_all` und der Arbeitsged√§chtnisleistung (Feature Switch Detection Task - `cvstm_propcorrect`) zu spezifizieren.

```{r, echo = TRUE, eval = FALSE}

reg_fit <- lm(Cognitive_Offloading_Variable ~ Arbeitsged√§chtnisleistung, data = dat_ full)

```

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

reg_fit <- lm(mean_rl_all ~ cvstm_propcorrect, data = dat_full)
```
:::

-   **Speichern der Residuen:** Speichere die Residuen dieses Regressionsmodells mit der Funktion `rstandard()`

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

residuals <- rstandard(reg_fit)
```
:::

-   **Visualisierung mittels Histogramm:** Stelle die die Residuen in einem Histogramm mit der Funktion `hist()` dar.

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}
hist(residuals)

```
:::

```         
*Frage:* Ist im Histogramm eine **Glockenkurve** (Normalverteilung) erkennbar?
```

-   **Visualisierung mittels QQ-Plot:** Stelle die Residuen in einem `QQ-Plot` mit der Funktion `qqnorm()` dar. Erg√§nze die **Regressionsgerade** mit `qqline()`.

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

qqnorm(residuals)
qqline(residuals)
```
:::

```         
*Frage:* Wie nah sind die Residuen einer **perfekten Normalverteilung** (d.h. der Geraden)?
```

## Skalenreliabilit√§t

üìñ [Psychometrics in R](https://bookdown.org/annabrown/psychometricsR/exercise5.html) & [Bj√∂rn Walther](https://bjoernwalther.com/cronbachs-alpha-in-r-berechnen/)

Die folgenden Schritte dienen der Pr√ºfung der ***internen Konsistenz*** der Messinstrumente.

-   **Datenvorbereitung f√ºr die Analyse:** Speichere alle Variablen des `MMQs` in einem **neuen** Data Frame ab. W√§hle dabei die Spalten mittels `select` aus.

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

mmq_vars <- dat_full |>
  select(starts_with("question"))

```
:::

-   **Berechnung von Cronbach's Alpha:** Verwende die Funktion `alpha()` aus dem `psych`-Paket und wende sie auf das neue Objekt an, um **Cronbach's Alpha** (Interne Konsistenz) f√ºr diesen Fragebogen zu berechnen. Schau dir die Ergebnisse (insbesondere `raw_alpha`) an. Dieses sollte mit Werten in @grinschgl2021 √ºbereinstimmen.

::: callout-warning
Achtung: Wenn du das Package ggplot() geladen hast, kann es sein dass die Funktion `alpha()`dadurch verdeckt wird. Spezifiziere aus welchem Package du die Funktion verwenden willst mit `psych::alpha()`
:::

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

psych::alpha(mmq_vars)
```
:::

-   Verwende die Funktion `omega()` aus dem **psych-Paket** und wende sie auf dein Objekt (z. B. den Datensatz mit den relevanten Items) an, um **McDonald‚Äôs Omega** (interne Konsistenz auf Basis eines Faktorenmodells) zu berechnen.\

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

psych::omega(mmq_vars)

```
:::

::: {.callout-note collapse="true"}
## PDF Grinschgl2021 ‚Äì\> Seite 7 ‚Äì\> Multifactorial Memory Questionnaire

<iframe src="../../PDFs/Grinschgl2020.pdf" width="100%" height="500px">

</iframe>
:::

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

alpha(mmq_vars)

```
:::

> **Tipp:** In der `alpha()` Funktion werden unter `details` alle Werte beschrieben. F√ºr weitere Erkl√§rungen siehe auch: [Psychometrics in R](https://bookdown.org/annabrown/psychometricsR/exercise5.html)

------------------------------------------------------------------------

## Streudiagramme/Scatterplots & weitere Datenverteilungen

| **Geom** | **Funktion** | **Einsatzbereich** |
|------------------------|------------------------|------------------------|
| geom_point() | Streudiagramm | Zwei numerische Variablen vergleichen (x = Var1, y = Var2) |
| geom_line() | Liniendiagramm | Werteverlauf √ºber kontinuierliche x-Achse darstellen |
| geom_bar() | Balkendiagramm | H√§ufigkeiten oder Aggregationen f√ºr Kategorien (x = Gruppe) |
| geom_histogram() | Histogramm | Verteilung einer numerischen Variable (x = Wert) |
| geom_boxplot() | Boxplot | Verteilungen √ºber Gruppen vergleichen (x = Gruppe, y = Wert) |

::: {.callout-note collapse="true"}
## Cheatsheet ggplot2

<iframe src="../../PDFs/ggplot.pdf" width="100%" height="500px">

</iframe>
:::

Um Plots zu erstellen gibt es diverse Packages. F√ºr einfache Darstellungen ist die Syntax der Base r `plot()` Funktion n√ºtzlich, bietet jedoch wesentlich weniger Optionen an als `ggplot()`. F√ºr diese aufgaben sind zwar beide geeignet, jedoch empfehlen wir `ggplot()`

-   Plotte den Zusammenhang zwischen Cognitive Offloading (√ñffnungen des Modelfensters - `mean_rl_all`) und der Arbeitsged√§chtnisleistung (Feature Switch Detection Task `cvstm_propcorrect`). Erg√§nze daf√ºr den Code unten mit den richtigen Variablen. (Wichtig f√ºr Ausrei√üerkontrolle bei Berechnungen von Korrelationen/Regressionen)

```{r, echo = TRUE, eval = FALSE}
ggplot(data = dat_full, aes(x = XXXXX, y = XXXXXX))+
  geom_XXXX()
```

-   Ausrei√üer k√∂nnen auch mit `boxplots` f√ºr einzelne Variablen identifiziert werden üëâ f√ºr √ñffnungen des Modelfensters anschauen. Nutze das richtige `geom()` aus der Tabelle.

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}
ggplot(data = dat_full, aes(y = mean_rl_all))+
  geom_boxplot()
```
:::

‚ùóMan sollte schon vor Studienerhebung oder zumindest vor der Datenanalyse festlegen ob bzw. wie man Ausrei√üer identifiziert und ausschlie√üt (z.B. in Pr√§registrierungen oder Datenanalysepl√§nen)

-   Lass dir den `boxplot()` getrennt f√ºr die 3 Experimentalgruppen anzeigen. Setze daf√ºr `y = group`.

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}
ggplot(data = dat_full, aes(x = group_all, y = mean_rl_all)) +
  geom_boxplot()

```
:::

## Fortgeschrittene (freiwillige √úbungen)

-   Erweitere den Scatterplot deiner beiden Variablen um die folgenden Dinge:

    -   F√§rbe deine Datenpunkte nach Gruppenzugeh√∂rigkeit ein mit `color()`

    -   F√ºge einen titel ein mit `ggtitle()`

    -   Benne die Achsen mit `ylab()` und `xlab()`

    -   √Ñnder die Darstellungart zu `theme_minimal()`

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}
ggplot(data = dat_full, aes(x = mean_rl_all, y = cvstm_propcorrect, color = group_all))+
  geom_point()+
  theme_minimal()+
  ggtitle("Meine Wundersch√∂ne Darstellung")+
  ylab("Arbeitsged√§chtnisleistung")+
  xlab("√ñffnungen des Modellfensters")
```
:::

# Tabellen

Diese Schritte dienen der Erstellung eines √ºbersichtlichen Tabellen mit deskriptiver Statistik.

-   **Paketinstallation und Laden:** Paket `knitr` **installieren** und **laden** (ggf. muss davor das Paket `xfun` geupdated werden.

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

#install.packages("knitr")
library(knitr)

```
:::

-   **Erstellung des Data Frames:** Erstelle einen Data Frame mit deskriptiver Statistik, z.B. mittels `summarise()` f√ºr die 4 Variablen des Pattern Copy Task

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}
summary_table <- dat_full |>
  group_by(group_all) |>
  summarize("Mean Openings of Model Window" = mean(mean_rl_all),
            "Mean Initially correctly copied items"  = mean(mean_e1_all),
            "Mean Initial encoding duration (sec)" = mean(mean_d1_all),
            "Mean Trial Duration (sec)" = mean(mean_d_all))


summary_table
```
:::

-   **Anwendung von Labels:** Wende `kable()` auf den Data Frame mit deskriptiver Statistik an.

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

knitr::kable(summary_table)
```
:::

-   **Hilfefunktion und Formatierung:** Schau dir die Hilfefunktion an und nimm Formatierungen vor, wie etwa die Umbenennung der Spalten mit `col.names(c(‚Äûnew colnames here ‚Äú))`, die Anpassung der Nachkommastellen auf 2 mit `digit =`, das Erg√§nzen einer √úberschrift mit `caption =`

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

knitr::kable(
  summary_table,
  col.names = c("Gruppe", "Durschschnittliche Modell√∂ffnungen", "Durschnittliche Korrekt kopierte Items", "Durschnittliche Enkodierungszeit", "Durschnittliche Trialdauer"),   # Spaltennamen
  digits = 2,                                                # 2 Nachkommastellen
  caption = "Deskriptivstatistik Cognitive Offloading")      
```
:::

------------------------------------------------------------------------

# APA-konforme Tabellen von statistischen Analysen

**üìëReferenz:** <https://dstanley4.github.io/apaTables/articles/apaTables.html>

-   **Paketinstallation und Laden:** Paket `apaTables` **installieren** und **laden**.

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

# install.packages("apaTables")
library(apaTables)

```
:::

-   **Datenvorbereitung:** Erstelle einen Data Frame nur mit den **Proportion Correct Variablen** der 3 Arbeitsged√§chtnistests.

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}
prop_correct_vars <- dat_full |> 
  select(ends_with("correct"))
```
:::

-   **Erstellung der Korrelationstabelle:** Wende `apa.cor.table()` auf diesen Data Frame an ‚Üí du bekommst eine **Korrelationstabelle im** APA Format.

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = TRUE}

apa.cor.table(prop_correct_vars)
```
:::

-   **Speichern der Tabelle:** Speichere die Tabelle in einer Word Datei mit der Erg√§nzung `filename = "somefilename.doc"` im Ordner outputs?

::: {.callout-note collapse="true" title="L√∂sung"}
```{r, echo = TRUE, eval = FALSE}
apa.cor.table(prop_correct_vars,
              filename = "propcorrtable.doc")
```
:::

> **Wichtiger Hinweis:** Diese Funktion kann man auch f√ºr Regressionen und ANOVAs erstellen. Wir benutzen aber ein anderes Paket f√ºr ANOVAs (nicht ezANOVA, auf dem apaTables basiert)!

# Am Ende deiner √úbungen - vergiss nicht dein Skript abzuspeichern! üòâ
