---
title: "Hands On ‚Äì Coding Basics (Einheiten 9 und 10)"
editor: visual
bibliography: references.bib
---

```{r, echo = FALSE, results = 'hide', message= FALSE, warning=FALSE}
library(tidyverse)

dat_full <- read_csv("data/raw/dat_full.csv")
```

Bei Bedarf finden sich hier nochmal die Slides zur EH9:

::: {=html}
<iframe src="../01_slides/EH_9.html"
        width="100%"
        height="500"
        style="border:0; display:block; margin: 0 0 2rem 0;"></iframe>
:::

Und hier die Slides zur EH10:

::: {=html}
<iframe src="../01_slides/EH_10.html"
        width="100%"
        height="500"
        style="border:0; display:block; margin: 0 0 2rem 0;"></iframe>
:::

# Lernziele

‚úÖ Dplyr: Repetition

‚úÖ Duplizierte Werte Entfernen

‚úÖ Long & Wide Transformationen

‚úÖ Datenqualit√§tsindkatoren berechnen

‚úÖ Ausreisseranalysen

‚úÖ Tabellen

::: callout-important
F√ºr die heutigen √úbungen ben√∂tigen wir `dat_full`!
:::

# **Duplizierte Werte entfernen**

In vielen Datens√§tzen kann es vorkommen, dass Werte mehrfach auftreten (z. B. wenn eine ID mehrmals vorkommt). Um dies zu bereinigen, m√ºssen wir zun√§chst die Duplikate identifizieren und sie anschliessend bearbeiten oder entfernen.

Im n√§chsten Schritt duplizieren wir exemplarisch drei zuf√§llige Zeilen aus unserem Datensatz **dat_full**. Diese k√ºnstlich erzeugten Duplikate werden wir danach wieder bereinigen.

F√ºhre diesen Code-Chunk aus: Danach sollte dein **dat_full** 162 Beobachtungen enthalten.

```{r}
set.seed(123)   

dup_rows <- dat_full %>% 
  slice_sample(n = 3)

dat_full <- bind_rows(dat_full, dup_rows)

```

Es gibt mehrere Arten und Weisen, Duplikate zu identifizieren. Verwende die `dplyr`-Funktion `count()`, um die einzelnen IDs zu z√§hlen. Gib dir anschliessend mit `filter()` diejenigen aus, die mehr als einmal vorkommen.

```{r echo = TRUE, eval = FALSE}
dat_full |> 
  count(XXXX) |> 
  filter(XXX)


```

```{r echo = FALSE, eval = FALSE}
dat_full |>
  count(code) |> 
  filter(n > 1)
```

Entferne die Duplikate wieder, indem du z. B. `unique()` auf den Datensatz anwendest. `unique()` entfernt vollst√§ndig identische Zeilen und beh√§lt nur eine davon.

```{r, echo = FALSE}
dat_full <- unique(dat_full)
```

::: {.callout-note collapse="true"}
Alternative: Falls du Duplikate nur auf Basis einer bestimmten Variable (z. B. id) entfernen m√∂chtest, kannst du duplicated() verwenden. Dadurch bleibt pro ID nur die erste Zeile erhalten.

```{r echo = TRUE, eval = FALSE}
dat_full <- dat_full[!duplicated(dat_full$code), ]

```
:::

# Dplyr: Code korrigieren und verbessern

**Dieser Code funktioniert nicht. Korrigiere ihn. Achte dabei auf Syntaxfehler.**

```{r, echo = TRUE, eval=FALSE}
dat_full %>%
  select(code, post, strategies)
  filter(strtegies = "1")



```

```{r echo = FALSE, eval = FALSE}
dat_full %>%
  select(code, post, strategies) %>%
  filter(strategies == 1)
```

**Dieser Code funktioniert, ist jedoch sehr umst√§ndlich und ineffizient. Verwende `dplyr`-Funktionen, um ihn k√ºrzer und √ºbersichtlicher zu gestalten.**

```{r echo = TRUE, eval=FALSE}
dat_full |> 
  summarise(
    Gesamtmittelwert = mean(
      c(
        question1, question2, question3, question4, question5,
        question6, question7, question8, question9, question10,
        question11, question12, question13, question14, question15,
        question16, question17, question18
      )
    )
  )

```

**üí°Tips:**

-   Nutze `select()` zusammen mit `starts_with()`.\
    Achtung: `select()` braucht als erstes Argument den Datensatz obwohl wir die Pipe verwenden.

-   Um den Gesamtmean zu berechnen, musst du zuerst `rowMeans()` anwenden und dann mit `mean()` kombinieren. `select()` gibt als Resultat nur ein Dataframe zur√ºck, das `mean()` nicht direkt verarbeiten kann.

Du kannst versuchen diese Struktur mit den richtigen Funktionen zu erg√§nzen.

```{r, echo = TRUE, eval = FALSE}

dat_full |>
  summarise(
    Gesamtmittelwert = Funktion_1(
      Funktion_2(
        Funktion_3(dat_full, starts_with("XXX"))
      )
    )
  )

```

```{r, echo = FALSE, eval=FALSE}

dat_full |> summarise(Gesamtmean = mean(rowMeans(select(dat_full, starts_with("question")))))

#Alternative

dat_full |> summarise(Gesamtmean = mean(c_across(starts_with("question"))))
```

# Wide to Long:

## √úbungen zur Datenkonversion

üìñ[R4DS - Kapitel 5.3](https://r4ds.hadley.nz/data-tidy.html#sec-pivoting)

üìñ[Einf√ºhrung in R - Kapitel 3.1.3](https://methodenlehre.github.io/einfuehrung-in-R/chapters/03-data_frames.html#manuelle-konversion-von-wide-zu-long)

Die folgenden Schritte dienen der Konversion von Daten zwischen dem Wide- und Long-Format unter Verwendung des `Tidyverse`-Pakets in R.

-   Mit der Funktion **`pivot_longer()`** Spalten der Leistungseinsch√§tzung in Long-Format umwandeln und in neuen Datensatz speichern, **nur f√ºr √úbungszwecke**.

    ```{r echo = FALSE, eval=FALSE}
    df_long <- dat_full |>
      pivot_longer(
        cols = c(pre1, pre2, pre3, pre4, post),
        names_to = "time_rating",
        values_to = "rating"
      )
    ```

-   Wandle den Datensatz noch mal in das Long Format um, aber nur basierend auf den Spalten **`pre1`** und **`pre4`** **Long-Format wird f√ºr 2x3 ANOVA aus Grinschgl et al. (2020) ben√∂tigt; f√ºr Vergleich der 2 Messzeitpunkte in dieser Analysen, ben√∂tigen wir diese im Long-Format.**

    ```{r, echo=FALSE, eval=FALSE}
    df_long <- dat_full |>
      pivot_longer(
        cols = c(pre1, pre4),
        names_to = "time_rating",
        values_to = "rating"
      )
    ```

-   Wandle die Spalte **`time_rating`** in einen Factor um ‚Äì\> **auch notwendig f√ºr Analysen**.

    ```{r, echo=FALSE, eval=FALSE}

    df_long$time_rating <- factor(df_long$time_rating)
    ```

-   Speichere den Long Datensatz als **`csv.`** Datei ab. Den Long Datensatz werden wir in den kommenden Wochen ben√∂tigen.

```{r, echo=FALSE, eval=FALSE}

write.csv(df_long, "processed/data_long.csv", row.names = FALSE)

```

-   Verwende **`pivot_wider`** unter Angabe von **`names_from`** und **`values_from`** um den Datensatz wieder in das Wide-Format zu bringen. ‚Äì\> **nur f√ºr √úbungszwecke.**

```{r, echo=FALSE, eval=FALSE}

df_wide <- df_long |> 
  pivot_wider(names_from = time_rating,
              values_from = rating)
```

# Datenqualit√§t

Ab hier wieder mit dem normalen Datensatz (`dat_full`) arbeiten.

## Skewness und Kurtosis

Die folgenden Schritte dienen der Vorbereitung und deskriptiven Analyse von Daten in R.

-   **Installieren und laden des Pakets** {psych}: Stelle sicher, dass das Statis Paket `psych`f√ºr die weiteren Analysen geladen ist.

```{r, echo=FALSE, eval=FALSE}

#install.packages("psych")
library(psych)
```

-   Berechne die Schiefe und die Kurtosis der Variablen "√ñffnungen des Modellfensters" (`mean_rl_all`) f√ºr die drei Gruppen. Verwende hierf√ºr die `dplyr`-Funktionen `group_by()` und `summarize()`.

```{r, echo=FALSE, eval=FALSE}

summary_table <- dat_full |>
  group_by(group_all) |>
  summarize(schiefe = skew(mean_rl_all),
            w√∂lbung = kurtosi(mean_rl_all))

summary_table
```

## Residuen

üìñSiehe auch: [Normalverteilung der Residuen bei der Regression in R testen](https://bjoernwalther.com/normalverteilung-der-residuen-bei-der-regression-in-r-testen/)

F√ºr die folgenden Schritte berechnen wir zuerst ein Regressionsmodell:

-   **Spezifikation des Regressionsmodells:** Verwende die Funktion `lm()` um ein Regressionsmodell zwischen Cognitive Offloading (`mean_rl_all` und der Arbeitsged√§chtnisleistung (Feature Switch Detection Task - `cvstm_propcorrect`) zu spezifizieren.

```{r, echo = TRUE, eval= FALSE}

reg_fit <- lm(Cognitive_Offloading_Variable ~ Arbeitsged√§chtnisleistung, data = dat_ full)

```

```{r, echo=FALSE, eval=FALSE}

reg_fit <- lm(mean_rl_all ~ cvstm_propcorrect, data = dat_full)
```

-   **Speichern der Residuen:** Speichere die Residuen dieses Regressionsmodells mit der Funktion `rstandard()`

```{r, echo=FALSE, eval=FALSE}

residuals <- rstandard(reg_fit)
```

-   **Visualisierung mittels Histogramm:** Stelle die die Residuen in einem Histogramm mit der Funktion `hist()` dar.

    ```{r, echo=FALSE, eval=FALSE}
    hist(residuals)

    ```

    *Frage:* Ist im Histogramm eine **Glockenkurve** (Normalverteilung) erkennbar?

-   **Visualisierung mittels QQ-Plot:** Stelledie Residuen in einem `QQ-Plot` mit der Funktion `qqnorm()` dar. Erg√§nze die **Regressionsgerade** mit `qqline()`.

    ```{r, echo=FALSE, eval=FALSE}

    qqnorm(residuals)
    qqline(residuals)
    ```

    *Frage:* Wie nah sind die Residuen einer **perfekten Normalverteilung** (d.h. der Geraden)?

## Skalenreliabilit√§t

üìñ [Psychometrics in R](https://bookdown.org/annabrown/psychometricsR/exercise5.html) & [Bj√∂rn Walther](https://bjoernwalther.com/cronbachs-alpha-in-r-berechnen/)

Die folgenden Schritte dienen der Pr√ºfung der ***internen Konsistenz*** der Messinstrumente.

-   **Datenvorbereitung f√ºr die Analyse:** Speichere alle Variablen des `MMQs` in einem **neuen** Data Frame ab. W√§hle dabei die Spalten mittels `select` aus.

    ```{r, echo=FALSE, eval=FALSE}

    mmq_vars <- dat_full |>
      select(starts_with("question"))

    ```

-   **Berechnung von Cronbach's Alpha:** Verwende die Funktion `alpha()` aus dem `psych`-Paket und wende sie auf das neue Objekt an, um **Cronbach's Alpha** (Interne Konsistenz) f√ºr diesen Fragebogen zu berechnen. Schau dir die Ergebnisse (insbesondere `raw_alpha`) an. Dieses sollte mit Werten in @grinschgl2021 √ºbereinstimmen.

::: callout-warning
Achtung: Wenn du das Package ggplot() geladen hast, kann es sein dass die Funktion `alpha()`dadurch verdeckt wird. Spezifiziere aus welchem Package du die Funktion verwenden willst mit `psych::alpha()`
:::

```{r, echo=FALSE, eval=FALSE}

psych::alpha(mmq_vars)
```

-   Verwende die Funktion `omega()` aus dem **psych-Paket** und wende sie auf dein Objekt (z. B. den Datensatz mit den relevanten Items) an, um **McDonald‚Äôs Omega** (interne Konsistenz auf Basis eines Faktorenmodells) zu berechnen.\

    ```{r, echo=FALSE, eval=FALSE}

    psych::omega(mmq_vars)

    ```

::: {.callout-note collapse="true"}
## PDF Grinschgl2021 ‚Äì\> Seite 7 ‚Äì\> Multifactorial Memory Questionnaire

<iframe src="../../PDFs/Grinschgl2020.pdf" width="100%" height="500px">

</iframe>
:::

```{r, echo=FALSE, eval=FALSE}

alpha(mmq_vars)

```

> **Tipp:** In der `alpha()` Funktion werden unter `details` alle Werte beschrieben. F√ºr weitere Erkl√§rungen siehe auch: [Psychometrics in R](https://bookdown.org/annabrown/psychometricsR/exercise5.html)

------------------------------------------------------------------------

## Streudiagramme/Scatterplots & weitere Datenverteilungen

üìä[Auflistung von Argumenten](https://ggplot2.tidyverse.org/reference/index.html)

üìä[R4DS - Layers](https://r4ds.hadley.nz/layers)

üìä[R4DS - EDA](https://r4ds.hadley.nz/EDA.html)

### Tabelle mit Geoms()

| Geom | Funktion | Einsatzbereich | Bild / Beispiel |
|------------------|------------------|------------------|------------------|
| **geom_point()** | Streudiagramm | Zwei numerische Variablen vergleichen (x = Var1, y = Var2) | ![](images/EH_10/geom_point.png) |
| **geom_jitter()** | Jitter-Plot | Punkte leicht versetzen, um √úberlappung zu vermeiden (v. a. bei kategorialem x) | ![](images/EH_10/geom_jitter.png) |
| **geom_line()** | Liniendiagramm | Werteverlauf √úber kontinuierliche x-Achse darstellen | ![](images/EH_10/geom_line.png) |
| **geom_bar()** | Balkendiagramm | H√§ufigkeiten oder Aggregationen f√ºr Kategorien (x = Gruppe) | ![](images/EH_10/geom_bar.png) |
| **geom_histogram()** | Histogramm | Verteilung einer numerischen Variable (x = Wert) | ![](images/EH_10/geom_histogram.png) |
| **geom_boxplot()** | Boxplot | Verteilungen √ºber Gruppen vergleichen (x = Gruppe, y = Wert) | ![](images/EH_10/geom_boxplot.png) |

::: {.callout-note collapse="true"}
## Cheatsheet ggplot2

<iframe src="../../PDFs/ggplot.pdf" width="100%" height="500px">

</iframe>
:::

Um Plots zu erstellen gibt es diverse Packages. F√ºr einfache Darstellungen ist die Syntax der Base r `plot()` Funktion n√ºtzlich, bietet jedoch wesentlich weniger Optionen an als `ggplot()`. F√ºr diese aufgaben sind zwar beide geeignet, jedoch empfehlen wir `ggplot()`

-   Plotte den Zusammenhang zwischen Cognitive Offloading (√ñffnungen des Modelfensters - `mean_rl_all`) und der Arbeitsged√§chtnisleistung (Feature Switch Detection Task `cvstm_propcorrect`). Erg√§nze daf√ºr den Code unten mit den richtigen Variablen. (Wichtig f√ºr Ausrei√üerkontrolle bei Berechnungen von Korrelationen/Regressionen)

```{r, echo = TRUE, eval=FALSE}
ggplot(data = dat_full, aes(x = XXXXX, y = XXXXXX))+
  geom_XXXX()
```

-   Ausrei√üer k√∂nnen auch mit `boxplots` f√ºr einzelne Variablen identifiziert werden üëâ f√ºr √ñffnungen des Modelfensters anschauen. Nutze das richtige `geom()` aus der Tabelle.

```{r, echo=FALSE, eval=FALSE}
ggplot(data = dat_full, aes(y = mean_rl_all))+
  geom_boxplot()
```

‚ùóMan sollte schon vor Studienerhebung oder zumindest vor der Datenanalyse festlegen ob bzw. wie man Ausrei√üer identifiziert und ausschlie√üt (z.B. in Pr√§registrierungen oder Datenanalysepl√§nen)

-   Lass dir den `boxplot()` getrennt f√ºr die 3 Experimentalgruppen anzeigen. Setze daf√ºr `y = group`.

```{r, echo=FALSE, eval=FALSE}
ggplot(data = dat_full, aes(x = group_all, y = mean_rl_all)) +
  geom_boxplot()

```

## Fortgeschrittene (freiwillige √úbungen)

-   Erweitere den Scatterplot deiner beiden Variablen um die folgenden Dinge:

    -   F√§rbe deine Datenpunkte nach Gruppenzugeh√∂rigkeit ein mit `color()`

    -   F√ºge einen titel ein mit `labs` und title()

    -   Benne die Achsen mit `ylab()` und `xlab()`

    -   √Ñnder die Darstellungart zu `theme_minimal()`

```{r, echo = FALSE, eval=FALSE}
ggplot(data = dat_full, aes(x = mean_rl_all, y = cvstm_propcorrect, color = group_all))+
  geom_point()+
  theme_minimal()+
  ggtitle("Meine Wundersch√∂ne Darstellung")+
  ylab("Arbeitsged√§chtnisleistung")+
  xlab("√ñffnungen des Modellfensters")
```

***Wir werden in EH 11 noch vertiefende √úbungen zu ggplot durchf√ºhren.***

# Tabellen

Diese Schritte dienen der Erstellung eines √ºbersichtlichen Tabellen mit deskriptiver Statistik.

-   **Paketinstallation und Laden:** Paket `knitr` **installieren** und **laden** (ggf. muss davor das Paket `xfun` geupdated werden.

```{r, echo=FALSE, eval=FALSE}

#install.packages("knitr")
library(knitr)

```

-   **Erstellung des Data Frames:** Erstelle einen Data Frame mit deskriptiver Statistik, z.B. mittels `summarise()` f√ºr die 4 Variablen des Pattern Copy Task

    ```{r, echo=FALSE, eval=FALSE}
    summary_table <- dat_full |>
      group_by(group_all) |>
      summarize("Mean Openings of Model Window" = mean(mean_rl_all),
                "Mean Initially correctly copied items"  = mean(mean_e1_all),
                "Mean Initial encoding duration (sec)" = mean(mean_d1_all),
                "Mean Trial Duration (sec)" = mean(mean_d_all))


    summary_table
    ```

-   **Anwendung von Labels:** Wende `kable()` auf den Data Frame mit deskriptiver Statistik an.

    ```{r, echo=FALSE, eval=FALSE}

    knitr::kable(summary_table)
    ```

-   **Hilfefunktion und Formatierung:** Schau dir die Hilfefunktion von `kable` an und nimm Formatierungen vor, wie etwa die Umbenennung der Spalten mit `col.names(c(‚Äûnew colnames here‚Äú))`, die Anpassung der Nachkommastellen auf 2 mit `digit =`, das Erg√§nzen einer √úberschrift mit `caption =`

```{r, echo=FALSE, eval=FALSE}

knittr::kable(
  summary_table,
  col.names = c("Gruppe", "Durschschnittliche Modell√∂ffnungen", "Durschnittliche Korrekt kopierte Items", "Durschnittliche Enkodierungszeit", "Durschnittliche Trialdauer"),   # Spaltennamen
  digits = 2,                                                # 2 Nachkommastellen
  caption = "Deskriptivstatistik Cognitive Offloading")      
```

------------------------------------------------------------------------

# APA-konforme Tabellen von statistischen Analysen

**üìëReferenz:** <https://dstanley4.github.io/apaTables/articles/apaTables.html>

-   **Paketinstallation und Laden:** Paket `apaTables` **installieren** und **laden**.

```{r, echo=FALSE, eval=FALSE}

#install.packages("apaTables")
library(apaTables)

```

-   **Datenvorbereitung:** Erstelle einen Data Frame nur mit den **Proportion Correct Variablen** der 3 Arbeitsged√§chtnistests.

```{r, echo=FALSE, eval=FALSE}
propcorrect_cortable <- prop_correct_vars <- dat_full |> 
  select(ends_with("correct"))
```

-   **Erstellung der Korrelationstabelle:** Wende `apa.cor.table()` auf diesen Data Frame an ‚Üí du bekommst eine **Korrelationstabelle im** APA Format.

```{r, echo=FALSE, eval=FALSE}

apa.cor.table(prop_correct_vars)
```

-   **Speichern der Tabelle:** Speichere die Tabelle in einer Word Datei mit der Erg√§nzung `filename = "somefilename.doc"` im Ordner *data -\> outputs -\> results*

```{r, echo=FALSE, eval=FALSE}
apa.cor.table(prop_correct_vars,
              filename = "propcorrtable.doc")
```

> **Wichtiger Hinweis:** Diese Funktion kann man auch f√ºr Regressionen und ANOVAs erstellen. Wir benutzen aber ein anderes Paket f√ºr ANOVAs (nicht ezANOVA, auf dem apaTables basiert)!

# Am Ende deiner √úbungen - vergiss nicht dein Skript abzuspeichern! üòâ
